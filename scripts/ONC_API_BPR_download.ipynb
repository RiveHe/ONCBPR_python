{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ec085e-eac8-4687-8b3b-19235c676e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from contextlib import closing\n",
    "import errno\n",
    "import math\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79264e-df65-409f-9727-f38792db18c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Id: 24792487\n",
      "Estimated File Size: 4 MB\n",
      "Estimated Processing Time: 5 s\n",
      "\n",
      "  Waiting... Running\n",
      "  Waiting... Querying data: 0% of time range completed.............\n",
      "  Downloading 1/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250326T000000Z_20250327T104319Z-clean.csv' (90.17 MB)\n",
      "  Downloading 2/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250327T104320Z_20250328T212640Z-clean.csv' (90.16 MB)\n",
      "  Downloading 3/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250328T212640Z_20250328T235959Z-clean.csv' (6.64 MB)\n",
      "Request Id: 24792491\n",
      "Estimated File Size: 4 MB\n",
      "Estimated Processing Time: 5 s\n",
      "\n",
      "  Waiting... Running\n",
      "  Waiting... Querying data: 0% of time range completed.............\n",
      "  Waiting... Search complete, waiting on the file system to synchronize (Endeavour_EndeavourEast_BottomPressureRecorder_20250329T000000Z_20250330T104320Z-clean.csv)..........\n",
      "  Downloading 1/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250329T000000Z_20250330T104320Z-clean.csv' (90.19 MB)\n",
      "  Downloading 2/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250330T104320Z_20250331T212640Z-clean.csv' (90.15 MB)\n",
      "  Downloading 3/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250331T212640Z_20250331T235959Z-clean.csv' (6.64 MB)\n",
      "Request Id: 24792500\n",
      "Estimated File Size: 4 MB\n",
      "Estimated Processing Time: 5 s\n",
      "\n",
      "  Waiting... Running.\n",
      "  Waiting... Querying data: 0% of time range completed...................\n",
      "  Waiting... Search complete, waiting on the file system to synchronize (Endeavour_EndeavourEast_BottomPressureRecorder_20250401T000000Z_20250402T104319Z-clean.csv)...\n",
      "  Downloading 1/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250401T000000Z_20250402T104319Z-clean.csv' (90.18 MB)\n",
      "  Downloading 2/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250402T104320Z_20250403T212640Z-clean.csv' (90.18 MB)\n",
      "  Downloading 3/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250403T212640Z_20250403T235959Z-clean.csv' (6.64 MB)\n",
      "Request Id: 24792506\n",
      "Estimated File Size: 4 MB\n",
      "Estimated Processing Time: 5 s\n",
      "\n",
      "  Waiting... Running\n",
      "  Waiting... Querying data: 0% of time range completed............\n",
      "  Waiting... Search complete, waiting on the file system to synchronize (Endeavour_EndeavourEast_BottomPressureRecorder_20250404T000000Z_20250405T104320Z-clean.csv)...........\n",
      "  Downloading 1/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250404T000000Z_20250405T104320Z-clean.csv' (90.15 MB)\n",
      "  Downloading 2/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250405T104320Z_20250406T212640Z-clean.csv' (90.17 MB)\n",
      "  Downloading 3/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250406T212640Z_20250406T235959Z-clean.csv' (6.64 MB)\n",
      "Request Id: 24792509\n",
      "Estimated File Size: 4 MB\n",
      "Estimated Processing Time: 5 s\n",
      "\n",
      "  Waiting... Running\n",
      "  Waiting... Querying data: 0% of time range completed...........\n",
      "  Waiting... Search complete, waiting on the file system to synchronize (Endeavour_EndeavourEast_BottomPressureRecorder_20250407T000000Z_20250408T104320Z-clean.csv)............\n",
      "  Downloading 1/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250407T000000Z_20250408T104320Z-clean.csv' (90.17 MB)\n",
      "  Downloading 2/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250408T104320Z_20250409T212640Z-clean.csv' (90.17 MB)\n",
      "  Downloading 3/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250409T212640Z_20250409T235959Z-clean.csv' (6.64 MB)\n",
      "Request Id: 24792515\n",
      "Estimated File Size: 4 MB\n",
      "Estimated Processing Time: 5 s\n",
      "\n",
      "  Waiting... Running\n",
      "  Waiting... Querying data: 0% of time range completed.............\n",
      "  Waiting... Search complete, waiting on the file system to synchronize (Endeavour_EndeavourEast_BottomPressureRecorder_20250410T000000Z_20250411T104320Z-clean.csv)..........\n",
      "  Downloading 1/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250410T000000Z_20250411T104320Z-clean.csv' (90.15 MB)\n",
      "  Downloading 2/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250411T104320Z_20250412T212640Z-clean.csv' (90.17 MB)\n",
      "  Downloading 3/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250412T212640Z_20250412T235959Z-clean.csv' (6.64 MB)\n",
      "Request Id: 24792518\n",
      "Estimated File Size: 4 MB\n",
      "Estimated Processing Time: 5 s\n",
      "\n",
      "  Waiting... Running\n",
      "  Waiting... Querying data: 0% of time range completed.............\n",
      "  Waiting... Search complete, waiting on the file system to synchronize (Endeavour_EndeavourEast_BottomPressureRecorder_20250413T000000Z_20250414T104320Z-clean.csv).........\n",
      "  Downloading 1/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250413T000000Z_20250414T104320Z-clean.csv' (90.16 MB)\n",
      "  Downloading 2/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250414T104320Z_20250415T212640Z-clean.csv' (90.15 MB)\n",
      "  Downloading 3/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250415T212640Z_20250415T235959Z-clean.csv' (6.63 MB)\n",
      "Request Id: 24792728\n",
      "Estimated File Size: 4 MB\n",
      "Estimated Processing Time: 5 s\n",
      "\n",
      "  Waiting... Running\n",
      "  Waiting... Querying data: 0% of time range completed.............\n",
      "  Waiting... Search complete, waiting on the file system to synchronize (Endeavour_EndeavourEast_BottomPressureRecorder_20250416T000000Z_20250417T104319Z-clean.csv).........\n",
      "  Downloading 1/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250416T000000Z_20250417T104319Z-clean.csv' (90.19 MB)\n",
      "  Downloading 2/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250417T104320Z_20250418T212640Z-clean.csv' (90.16 MB)\n",
      "  Downloading 3/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250418T212640Z_20250418T235959Z-clean.csv' (6.64 MB)\n",
      "Request Id: 24792742\n",
      "Estimated File Size: 4 MB\n",
      "Estimated Processing Time: 5 s\n",
      "\n",
      "  Waiting... Running\n",
      "  Waiting... Querying data: 0% of time range completed..................\n",
      "  Waiting... Search complete, waiting on the file system to synchronize (Endeavour_EndeavourEast_BottomPressureRecorder_20250419T000000Z_20250420T104320Z-clean.csv)....\n",
      "  Downloading 1/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250419T000000Z_20250420T104320Z-clean.csv' (90.15 MB)\n",
      "  Downloading 2/1 'Endeavour_EndeavourEast_BottomPressureRecorder_20250420T104320Z_20250421T212640Z-clean.csv' (90.17 MB)\n"
     ]
    }
   ],
   "source": [
    "url = 'https://data.oceannetworks.ca/api/dataProductDelivery'\n",
    "token = 'a7ec956d-ede0-4c8e-ad9d-0c5ce022bcb8' # replace YOUR_TOKEN_HERE with your personal token obtained from the 'Web Services API' tab at https://data.oceannetworks.ca/Profile when logged in.\n",
    "downloadFolder = r'H:\\Chapter3\\Endeavor\\ONC_API_BPR\\EndeavorEast' # The folder that file(s) will be saved to\n",
    "\n",
    "\n",
    "# replace YOUR_TOKEN_HERE with your personal token obtained from the 'Web Services API' tab at https://data.oceannetworks.ca/Profile when logged in.\n",
    "#requestParameters = {'method':'request',\n",
    "#                    'token':token,\n",
    "#                    'locationCode':'ENEF',                     # Barkley Canyon / Axis (POD 1) #https://wiki.oceannetworks.ca/spaces/O2A/pages/49447553/Available+Locations\n",
    "#                    'deviceCategoryCode':'BPR',            # 150 kHz Acoustic Doppler Current Profiler\n",
    "#                    'dataProductCode':'TSSD',                   # Time Series Scalar Data\n",
    "#                    'extension':'csv',                          # Comma Separated spreadsheet file\n",
    "#                    'dateFrom':'2022-03-01T00:00:00.000Z',         # The datetime of the first (From) data point\n",
    "#                    'dateTo':'2022-03-05T00:00:00.000Z',           # The datetime of the last (To) data point\n",
    "#                    'dpo_qualityControl':1,                     # The Quality Control data product option - See https://wiki.oceannetworks.ca/display/DP/1\n",
    "#                    'dpo_resample':'none',                      # The Resampling data product option - See https://wiki.oceannetworks.ca/display/DP/1\n",
    "#                    'dpo_dataGaps':0}                           # The Data Gaps data product option - See https://wiki.oceannetworks.ca/display/DP/1\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Set the start and end dates for the data requests\n",
    "    start_date = datetime.datetime(2025, 3, 26)\n",
    "    end_date = datetime.datetime(2025, 12, 31)\n",
    "    delta = datetime.timedelta(days=3)\n",
    "\n",
    "    # Loop over each time period\n",
    "    while start_date < end_date:\n",
    "        dateFrom = start_date.strftime('%Y-%m-%dT%H:%M:%S.000Z')\n",
    "        dateTo = (start_date + delta).strftime('%Y-%m-%dT%H:%M:%S.000Z')\n",
    "        \n",
    "        # Update requestParameters with the new dates\n",
    "        requestParameters = {\n",
    "            'method': 'request',\n",
    "            'token': token,\n",
    "            'locationCode': 'ENEF',\n",
    "            'deviceCategoryCode': 'BPR',\n",
    "            'dataProductCode': 'TSSD',\n",
    "            'extension': 'csv',\n",
    "            'dateFrom': dateFrom,\n",
    "            'dateTo': dateTo,\n",
    "            'dpo_qualityControl': 1,\n",
    "            'dpo_resample': 'none',\n",
    "            'dpo_dataGaps':0\n",
    "        }\n",
    "\n",
    "        # Request and download data\n",
    "        requestId = requestDataProduct(requestParameters)\n",
    "        if requestId:\n",
    "            runIds = runDataProduct(requestId)\n",
    "            for runId in runIds:\n",
    "                indx = 1\n",
    "                while True:\n",
    "                    dict = downloadDataProductIndex(runId, indx, downloadFolder)\n",
    "                    if dict:\n",
    "                        indx += 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "        # Update start_date to the next increment\n",
    "        start_date += delta\n",
    "def requestDataProduct(parameters):\n",
    "    response = requests.get(url,params=parameters)\n",
    "    requestId = None\n",
    "    if (response.ok):\n",
    "        requestInfo = json.loads(str(response.content,'utf-8')) # convert the json response to an object\n",
    "        requestId = requestInfo['dpRequestId']\n",
    "        print('Request Id: {}'.format(requestId))      # Print the Request Id\n",
    "        if ('numFiles' in requestInfo.keys()):\n",
    "            print('File Count: {}'.format(requestInfo['numFiles']))     # Print the Estimated File Size\n",
    "        if ('fileSize' in requestInfo.keys()):\n",
    "            print('File Size: {}'.format(requestInfo['fileSize']))      # Print the Estimated File Size\n",
    "        if 'downloadTimes' in requestInfo.keys():\n",
    "            print('Estimated download time:')\n",
    "            for e in sorted(requestInfo['downloadTimes'].items(),key=lambda t: t[1]):\n",
    "                print('  {} - {} sec'.format(e[0],'{:0.2f}'.format(e[1])))\n",
    "        if 'estimatedFileSize' in requestInfo.keys():\n",
    "            print('Estimated File Size: {}'.format(requestInfo['estimatedFileSize']))\n",
    "        if 'estimatedProcessingTime' in requestInfo.keys():\n",
    "            print('Estimated Processing Time: {}'.format(requestInfo['estimatedProcessingTime']))\n",
    "    else:\n",
    "        if(response.status_code == 400):\n",
    "            error = json.loads(str(response.content,'utf-8'))\n",
    "            print(error) # json response contains a list of errors, with an errorMessage and parameter\n",
    "        else:\n",
    "            print ('Error {} - {}'.format(response.status_code,response.reason))\n",
    "\n",
    "    return requestId\n",
    "def runDataProduct(requestId):\n",
    "    parameters = {'method':'run',\n",
    "                'token':token,\n",
    "                'dpRequestId':requestId}\n",
    "    response = requests.get(url,params=parameters)\n",
    "    runIds = []\n",
    "    if (response.ok):\n",
    "        r= json.loads(str(response.content,'utf-8')) # convert the json response to an object\n",
    "        runIds = [run['dpRunId'] for run in r]\n",
    "    else:\n",
    "        if(response.status_code == 400):\n",
    "            error = json.loads(str(response.content,'utf-8'))\n",
    "            print(error) # json response contains a list of errors, with an errorMessage and parameter\n",
    "        else:\n",
    "            print ('Error {} - {}'.format(response.status_code,response.reason))\n",
    "    return runIds\n",
    "def downloadDataProductIndex(runId,                     # The ID of the run process to download the files for. RunIds are returned from the dataProductDelivery run method\n",
    "                             indx=1,                    # The index of the file to be downloaded. Data files have an index of 1 or higher. The Metadata has an index of 'meta'\n",
    "                             outPath='c:/temp',\n",
    "                             fileCount=1,               # The actual or estimated file count, which is returned from the dataProductDelivery request method\n",
    "                             estimatedProcessingTime=1, # The estimated processing time in seconds, which is used to determine how often to poll the web service. The estimated processing time is returned from the dataProductDelivery request method\n",
    "                             maxRetries=100):           # Determines the maximum number of times the process will poll the service before it times out. The purpose of this property is to prevent hung processes on the Task server to hang this process.\n",
    "    parameters = {'method':'download',\n",
    "                'token':token,\n",
    " \t\t\t    'dpRunId':runId,\n",
    "                'index':indx}\n",
    "    defaultSleepTime = 2\n",
    "    downloadResult = {}\n",
    "    tryCount = 0\n",
    "    lastMessage = None\n",
    "    if (estimatedProcessingTime > 1):\n",
    "        sleepTime = estimatedProcessingTime * 0.5\n",
    "    else:\n",
    "        sleepTime = defaultSleepTime\n",
    "    while True:\n",
    "        tryCount+=1\n",
    "        if tryCount >= maxRetries:\n",
    "            msg = 'Maximum number of retries ({}) exceeded'.format(maxRetries)\n",
    "            print(msg)\n",
    "            break\n",
    "        with closing(requests.get(url,params=parameters,stream=True)) as streamResponse:\n",
    "            if (streamResponse.ok): #Indicates that the request was successful and did not fail. The status code indicates if the stream contains a file (200) or\n",
    "                if streamResponse.status_code == 200: #OK\n",
    "                    tryCount=0\n",
    "                    if 'Content-Disposition' in streamResponse.headers.keys():\n",
    "                        content = streamResponse.headers['Content-Disposition']\n",
    "                        filename = content.split('filename=')[1]\n",
    "                    else:\n",
    "                        print('Error: Invalid Header')\n",
    "                        streamResponse.close()\n",
    "                        break\n",
    "                    if 'Content-Length' in streamResponse.headers.keys():\n",
    "                        size = streamResponse.headers['Content-Length']\n",
    "                    else:\n",
    "                        size = 0\n",
    "                    filePath = '{}/{}'.format(outPath,filename)\n",
    "                    try:\n",
    "                        if (indx==1):\n",
    "                            print('')\n",
    "                        if (not os.path.isfile(filePath)):\n",
    "                            #Create the directory structure if it doesn't already exist\n",
    "                            try:\n",
    "                                os.makedirs(outPath)\n",
    "                            except OSError as exc:\n",
    "                                if exc.errno == errno.EEXIST and os.path.isdir(outPath):\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    raise\n",
    "                            if fileCount == 0:\n",
    "                                print (\"  Downloading {} '{}' ({})\".format(indx,filename,convertSize(float(size))))\n",
    "                            else:\n",
    "                                print (\"  Downloading {}/{} '{}' ({})\".format(indx,fileCount,filename,convertSize(float(size))))\n",
    "                            with open(filePath,'wb') as handle:\n",
    "                                try:\n",
    "                                    for block in streamResponse.iter_content(1024):\n",
    "                                        handle.write(block)\n",
    "                                except KeyboardInterrupt:\n",
    "                                    print('Process interupted: Deleting {}'.format(filePath))\n",
    "                                    handle.close()\n",
    "                                    streamResponse.close()\n",
    "                                    os.remove(filePath)\n",
    "                                    sys.exit(-1)\n",
    "                        else:\n",
    "                            if fileCount == 0:\n",
    "                                print (\"  Skipping {} '{}': File Already Exists\".format(indx,filename))\n",
    "                            else:\n",
    "                                print (\"  Skipping {}/{} '{}': File Already Exists\".format(indx,fileCount,filename))\n",
    "                    except:\n",
    "                        msg = 'Error streaming response.'\n",
    "                        print(msg)\n",
    "                    downloadResult['url'] = url\n",
    "                    streamResponse.close()\n",
    "                    break\n",
    "\n",
    "                elif streamResponse.status_code == 202: #Accepted - Result is not complete -> Retry\n",
    "                    payload = json.loads(str(streamResponse.content, 'utf-8'))\n",
    "                    if isinstance(payload, dict) and 'message' in payload:\n",
    "                             msg = payload['message']\n",
    "                             if msg != lastMessage:\n",
    "                                 print('\\n  Waiting... {}'.format(msg), end='')\n",
    "                                 sys.stdout.flush()\n",
    "                                 lastMessage = msg\n",
    "                                 tryCount = 0\n",
    "                             else:\n",
    "                                 print('.', end='', sep='')\n",
    "                                 sys.stdout.flush()\n",
    "                    else:\n",
    "                             print('\\n  Waiting... no message in payload. Retrying.')\n",
    "\n",
    "\n",
    "                elif streamResponse.status_code == 204: #No Content - No Data found\n",
    "                    responseStr = str(streamResponse.content,'utf-8')\n",
    "                    if not(responseStr == ''):\n",
    "                        payload = json.loads(responseStr)\n",
    "                        msg = '  {} [{}]'.format(payload['message'],streamResponse.status_code)\n",
    "                    else:\n",
    "                        msg = 'No Data found'\n",
    "                    print('\\n{}'.format(msg))\n",
    "                    streamResponse.close()\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    msg = 'HTTP Status: {}'.format(streamResponse.status_code)\n",
    "                    print(msg)\n",
    "\n",
    "            elif streamResponse.status_code == 400: #Error occurred\n",
    "                print('  HTTP Status: {}'.format(streamResponse.status_code))\n",
    "                payload = json.loads(str(streamResponse.content,'utf-8'))\n",
    "                if len(payload) >= 1:\n",
    "                    if ('errors' in payload):\n",
    "                        for e in payload['errors']:\n",
    "                            msg = e['errorMessage']\n",
    "                            printErrorMesasge(streamResponse,parameters)\n",
    "                    elif ('message' in payload):\n",
    "                        msg = '  {} [{}]'.format(payload['message'],streamResponse.status_code)\n",
    "                        print('\\n{}'.format(msg))\n",
    "                    else:\n",
    "                        print(msg)\n",
    "                else:\n",
    "                    msg = 'Error occurred processing data product request'\n",
    "                    print(msg)\n",
    "                streamResponse.close()\n",
    "                break\n",
    "            elif streamResponse.status_code == 404:  #Not Found - Beyond End of Index - Index # > Results Count\n",
    "                streamResponse.close()\n",
    "                downloadResult = None\n",
    "                break\n",
    "            elif streamResponse.status_code == 410: #Gone - file does not exist on the FTP server. It may not have been transfered to the FTP server  yet\n",
    "                payload = json.loads(str(streamResponse.content,'utf-8'))\n",
    "                if len(payload) >= 1:\n",
    "                    msg = payload['message']\n",
    "                    if (msg != lastMessage):\n",
    "                        print('\\n  Waiting... {}'.format(msg),end='')\n",
    "                        sys.stdout.flush()\n",
    "                        lastMessage=msg\n",
    "                        tryCount=0\n",
    "                    else:\n",
    "                        print('.',end='',sep='')\n",
    "                        sys.stdout.flush()\n",
    "                else:\n",
    "                    print('\\nRunning... Writing File.')\n",
    "            elif streamResponse.status_code == 500: #Internal Server Error occurred\n",
    "                msg = printErrorMesasge(streamResponse,parameters)\n",
    "                print('  URL: {}'.format(streamResponse.url))\n",
    "                streamResponse.close()\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    payload = json.loads(str(streamResponse.content,'utf-8'))\n",
    "                    if len(payload) >= 1:\n",
    "                        if ('errors' in payload):\n",
    "                            for e in payload['errors']:\n",
    "                                msg = e['errorMessage']\n",
    "                                printErrorMesasge(streamResponse,parameters)\n",
    "                        elif ('message' in payload):\n",
    "                            msg = payload['message']\n",
    "                            print('\\n  {} [{}]'.format(msg,streamResponse.status_code))\n",
    "                    streamResponse.close()\n",
    "                    break\n",
    "                except:\n",
    "                    printErrorMesasge(streamResponse,parameters)\n",
    "                    print('{} Retrying...'.format(msg))\n",
    "                    streamResponse.close()\n",
    "                    break\n",
    "\n",
    "        streamResponse.close()\n",
    "\n",
    "        if (tryCount <= 5) and (sleepTime > defaultSleepTime):\n",
    "            sleepTime = sleepTime * 0.5\n",
    "        time.sleep(sleepTime)\n",
    "\n",
    "    return downloadResult\n",
    "def convertSize(size):\n",
    "\n",
    "   if (size == 0):\n",
    "       return '0 KB'\n",
    "\n",
    "   size_name = (\"B\",\"KB\",\"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "   i = int(math.floor(math.log(size,1024)))\n",
    "   p = math.pow(1024,i)\n",
    "   s = round(size/p,2)\n",
    "\n",
    "   return '%s %s' % (s,size_name[i])\n",
    "def printErrorMesasge(response,\n",
    "                      parameters,\n",
    "                      showUrl=False,\n",
    "                      showValue=False):\n",
    "\n",
    "    if(response.status_code == 400):\n",
    "        if showUrl:print('Error Executing: {}'.format(response.url))\n",
    "\n",
    "        payload = json.loads(str(response.content,'utf-8'))\n",
    "\n",
    "        if len(payload) >= 1:\n",
    "            for e in payload['errors']:\n",
    "                msg = e['errorMessage']\n",
    "                parm = e['parameter']\n",
    "                matching = [p for p in parm.split(',') if p in parameters.keys()]\n",
    "                if len(matching) >=1:\n",
    "                    for p in matching:print(\"  '{}' for {} - value: '{}'\".format(msg,p,parameters[p]))\n",
    "                else:\n",
    "                    print(\"  '{}' for {}\".format(msg,parm))\n",
    "\n",
    "                if showValue:\n",
    "                    for p in parm.split(','):\n",
    "                        parmValue = parameters[p]\n",
    "                        print(\"  {} for {} - value: '{}'\".format(msg,p,parmValue))\n",
    "\n",
    "            return payload\n",
    "    else:\n",
    "        msg = 'Error {} - {}'.format(response.status_code,response.reason)\n",
    "        print (msg)\n",
    "        return msg\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    #print(f\"Failed to submit data processing request: {response.status_code} {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2942f38e-47d2-42a1-aea0-3c3536e5f77a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
